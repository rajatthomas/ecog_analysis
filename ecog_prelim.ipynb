{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_files = ['Japan_Patient7_PainRatingTask_InsulaB49.csv',\n",
    "                'Japan_Patient7_PainRatingTask_InsulaB50.csv',\n",
    "                'Japan_Patient7_PainRatingTask_InsulaB51.csv',\n",
    "                'Japan_Patient7_PainRatingTask_InsulaB52.csv',\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the csv files into numpy array\n",
    "#col_names = [f'BeforeXpression_{i}' for i in range(1000)] + [f'AfterXpression_{i}' for i in range(1000)] + ['StimType', 'StimIntensity', 'SubjRating' ]\n",
    "ntrials = 252\n",
    "ncols = 2003\n",
    "nelectrodes = 4\n",
    "\n",
    "all_data = np.zeros((ntrials, ncols, nelectrodes))\n",
    "for i, f in enumerate(rating_files):\n",
    "    all_data[:,:,i] = pd.read_csv(f, header=None).values\n",
    "\n",
    "np.save('patient7.npy', all_data)\n",
    "\n",
    "#df.index = [f'Trial_{i}' for i in range(252)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ecogDataset(Dataset):\n",
    "    def __init__(self, data_file, features='afterX', label_type='all', label= 'type', split='train'):\n",
    "        '''\n",
    "        data_file = '.npy file [ntrials X ncols[all features + labels] X nelectrodes ]'\n",
    "        features = 'beforeX' | 'afterX' | 'all'\n",
    "        label_type = 'color' | 'face' | 'hand' | 'all'\n",
    "        label = 'type' | 'stim_intensity' | 'rating'\n",
    "        split = 'train' | 'val' | 'test'\n",
    "        '''\n",
    "        \n",
    "        data = np.load(data_file)\n",
    "        \n",
    "        if features == 'beforeX':\n",
    "            select_features = range(0, 1000)\n",
    "        if features == 'afterX':\n",
    "            select_features = range(1000, 2000)\n",
    "        if features == 'all':\n",
    "            select_features = range(0, 2000)\n",
    "            \n",
    "            \n",
    "        \n",
    "        if label == 'type':\n",
    "            select_label = 2000\n",
    "            to_subtract = 1   # labels should start from 0[color], 1[face], 2[hand]\n",
    "            \n",
    "        if label == 'stim_intensity':\n",
    "            select_label = 2001\n",
    "            if label_type == 'color':\n",
    "                to_subtract = 2  # intensity values for color starts from 2\n",
    "            if label_type == 'face':\n",
    "                to_subtract = 1  # intensity values for face starts from 1\n",
    "            if label_type == 'hand':\n",
    "                to_subtract = 2  # intensity values for hand starts from 2\n",
    "            if label_type == 'all':\n",
    "                to_subtract = 1\n",
    "            \n",
    "        if label == 'rating':\n",
    "            select_label = 2002\n",
    "            to_subtract = 1\n",
    "        \n",
    "        if label_type == 'color':\n",
    "            data = data[data[:,2000,0]==1] # color stimuli are coded as 1\n",
    "        if label_type == 'face':\n",
    "            data = data[data[:,2000,0]==2]\n",
    "        if label_type == 'hand':\n",
    "            data = data[data[:,2000,0]==3]\n",
    "        \n",
    "        ntrials, _, _ = data.shape\n",
    "        ntrain = int(0.8 * ntrials)\n",
    "        nval = int(0.1 * ntrials)\n",
    "        \n",
    "        if split == 'train':\n",
    "            select_trials = range(0, ntrain)\n",
    "        if split == 'val':\n",
    "            select_trials = range(ntrain, ntrain+nval)\n",
    "        if split == 'test':\n",
    "            select_trials = range(ntrain+nval, ntrials)\n",
    "        \n",
    "        data = data[select_trials]\n",
    "        self.features = data[:, select_features, :]\n",
    "        self.label = data[:, select_label, 0] - to_subtract # start all labels to from 0 to NCLASSES-1\n",
    "    \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.features.shape[0] # num trials\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return np.transpose(self.features[index]), self.label[index]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rating1DConvNet(nn.Module):\n",
    "    def __init__(self, noutputs, nelectrodes=4):\n",
    "        super(Rating1DConvNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(nelectrodes, 16, 7)\n",
    "        self.conv2 = nn.Conv1d(16,32, 5)\n",
    "        self.conv3 = nn.Conv1d(32, 64, 5)\n",
    "        self.avg = nn.AdaptiveAvgPool1d((1))\n",
    "        \n",
    "        self.linear1 = nn.Linear(64, 100)\n",
    "        self.linear2 = nn.Linear(100, noutputs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.avg(x).view(-1, 64)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(net, val_data, criterion=nn.CrossEntropyLoss()):\n",
    "    \n",
    "    val_data_loader = DataLoader(val_data, batch_size=16, shuffle=True)\n",
    "    \n",
    "    net.eval()\n",
    "    loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(val_data_loader):\n",
    "\n",
    "                inputs = Variable(inputs).type(torch.FloatTensor)\n",
    "                labels = Variable(labels).type(torch.LongTensor)\n",
    "\n",
    "                # forward pass\n",
    "                output = net(inputs)\n",
    "\n",
    "                # calculate loss\n",
    "                loss += criterion(output, labels)\n",
    "\n",
    "    return loss/len(val_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(data_file, features='afterX', label_type='all', label= 'type', nepochs=100, verbose=True):\n",
    "    \n",
    "    \n",
    "    train_data = ecogDataset(data_file, features=features, label_type=label_type, label=label, split='train')\n",
    "    train_data_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "    \n",
    "    val_data = ecogDataset(data_file, features=features, label_type=label_type, label=label, split='val')\n",
    "\n",
    "    if label == 'type':\n",
    "        if label_type != 'all':\n",
    "            print('We have a problem ... label = Type and label_type != all cannot BE!!! ')\n",
    "    \n",
    "    if label == 'stim_intensity':\n",
    "        if label_type == 'color':\n",
    "            nclasses = 3                      # 2, 3, and 5 is the rating\n",
    "        if label_type == 'face':\n",
    "            nclasses = 6                      # 1, 2, 3, 4, 5, and 6 is the rating\n",
    "        if label_type == 'hand':\n",
    "            nclasses = 5                      # 2, 3, 4, 5, and 6 is the rating\n",
    "\n",
    "    if label == 'type':\n",
    "        nclasses = 3\n",
    "        \n",
    "    if label == 'rating':\n",
    "        nclasses = train_data[:][1].max()+1  # rating usually from 1 to 10 but subjects never report 10 \n",
    "        \n",
    "    # Setup the network, loss and optimizer    \n",
    "    net = Rating1DConvNet(nclasses, nelectrodes=4)    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=.001, weight_decay=0)\n",
    "    \n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    \n",
    "    print(f'Training ...')\n",
    "    for i_epoch in range(nepochs): \n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_data_loader):\n",
    "\n",
    "            inputs = Variable(inputs).type(torch.FloatTensor)    \n",
    "            labels = Variable(labels).type(torch.LongTensor)\n",
    "\n",
    "            # forward pass\n",
    "            output = net(inputs)\n",
    "\n",
    "            # calculate loss\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss\n",
    "\n",
    "        epoch_train_loss = epoch_loss/i\n",
    "        epoch_val_loss = validate_model(net, val_data, criterion=criterion)\n",
    "        \n",
    "        train_loss.append(epoch_train_loss)\n",
    "        val_loss.append(epoch_val_loss)\n",
    "        \n",
    "        if verbose and i_epoch%1 == 0:\n",
    "            print('Epoch:{} --- Train_loss:{} --- Val_loss:{}'.format(i_epoch, epoch_train_loss, epoch_val_loss))\n",
    "            \n",
    "    return net, train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(net, test_data):\n",
    "    \n",
    "    test_data_loader = DataLoader(test_data, batch_size=2, shuffle=False)\n",
    "    \n",
    "    net.eval()\n",
    "    tot_acc = 0.0\n",
    "    debug_outputs = []\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(test_data_loader):\n",
    "\n",
    "                inputs = Variable(inputs).type(torch.FloatTensor)\n",
    "                labels = Variable(labels).type(torch.LongTensor)\n",
    "\n",
    "                # forward pass\n",
    "                output = torch.argmax(net(inputs), dim=1)\n",
    "                debug_out.append(output)\n",
    "                # calculate accuracy\n",
    "                tot_acc += (output == labels).sum().item()\n",
    "\n",
    "    return tot_acc/len(test_data), debug_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n",
      "Epoch:0 --- Train_loss:1.2137571573257446 --- Val_loss:1.162784218788147\n",
      "Epoch:1 --- Train_loss:1.1741509437561035 --- Val_loss:1.0727427005767822\n",
      "Epoch:2 --- Train_loss:1.164929986000061 --- Val_loss:0.9834615588188171\n",
      "Epoch:3 --- Train_loss:1.1564332246780396 --- Val_loss:1.0735676288604736\n",
      "Epoch:4 --- Train_loss:1.1405385732650757 --- Val_loss:1.074924111366272\n",
      "Epoch:5 --- Train_loss:1.149308681488037 --- Val_loss:1.1095890998840332\n",
      "Epoch:6 --- Train_loss:1.1927982568740845 --- Val_loss:1.1461105346679688\n",
      "Epoch:7 --- Train_loss:1.1502342224121094 --- Val_loss:1.0408997535705566\n",
      "Epoch:8 --- Train_loss:1.1225734949111938 --- Val_loss:1.0234113931655884\n",
      "Epoch:9 --- Train_loss:1.1065424680709839 --- Val_loss:1.009441614151001\n",
      "Epoch:10 --- Train_loss:1.11799955368042 --- Val_loss:1.0586180686950684\n",
      "Epoch:11 --- Train_loss:1.121146559715271 --- Val_loss:1.052943468093872\n",
      "Epoch:12 --- Train_loss:1.0935885906219482 --- Val_loss:1.0113180875778198\n",
      "Epoch:13 --- Train_loss:1.09441339969635 --- Val_loss:1.0700544118881226\n",
      "Epoch:14 --- Train_loss:1.1040064096450806 --- Val_loss:0.9441683292388916\n",
      "Epoch:15 --- Train_loss:1.1166163682937622 --- Val_loss:0.9513808488845825\n",
      "Epoch:16 --- Train_loss:1.1113773584365845 --- Val_loss:1.1108940839767456\n",
      "Epoch:17 --- Train_loss:1.08775794506073 --- Val_loss:1.02988862991333\n",
      "Epoch:18 --- Train_loss:1.090283751487732 --- Val_loss:1.0318553447723389\n",
      "Epoch:19 --- Train_loss:1.099753737449646 --- Val_loss:0.9093660116195679\n",
      "Epoch:20 --- Train_loss:1.0833576917648315 --- Val_loss:0.9959750175476074\n",
      "Epoch:21 --- Train_loss:1.0807217359542847 --- Val_loss:1.002561330795288\n",
      "Epoch:22 --- Train_loss:1.0626658201217651 --- Val_loss:1.0034791231155396\n",
      "Epoch:23 --- Train_loss:1.1016573905944824 --- Val_loss:1.0857945680618286\n",
      "Epoch:24 --- Train_loss:1.1308091878890991 --- Val_loss:1.2045769691467285\n",
      "Epoch:25 --- Train_loss:1.1290777921676636 --- Val_loss:0.9975565075874329\n",
      "Epoch:26 --- Train_loss:1.0812486410140991 --- Val_loss:1.0123133659362793\n",
      "Epoch:27 --- Train_loss:1.0871220827102661 --- Val_loss:0.9959207773208618\n",
      "Epoch:28 --- Train_loss:1.0738648176193237 --- Val_loss:1.0480767488479614\n",
      "Epoch:29 --- Train_loss:1.0616815090179443 --- Val_loss:1.0934288501739502\n",
      "Epoch:30 --- Train_loss:1.0565824508666992 --- Val_loss:1.0169727802276611\n",
      "Epoch:31 --- Train_loss:1.0420053005218506 --- Val_loss:1.127036452293396\n",
      "Epoch:32 --- Train_loss:1.0505834817886353 --- Val_loss:1.083292841911316\n",
      "Epoch:33 --- Train_loss:1.0680301189422607 --- Val_loss:1.0156537294387817\n",
      "Epoch:34 --- Train_loss:1.0769954919815063 --- Val_loss:0.9333983659744263\n",
      "Epoch:35 --- Train_loss:1.0147994756698608 --- Val_loss:1.0617685317993164\n",
      "Epoch:36 --- Train_loss:1.0268980264663696 --- Val_loss:0.9750022292137146\n",
      "Epoch:37 --- Train_loss:1.0170618295669556 --- Val_loss:1.0361595153808594\n",
      "Epoch:38 --- Train_loss:0.9979037642478943 --- Val_loss:0.9724006056785583\n",
      "Epoch:39 --- Train_loss:1.0018192529678345 --- Val_loss:1.14524245262146\n",
      "Epoch:40 --- Train_loss:1.0035080909729004 --- Val_loss:1.1498851776123047\n",
      "Epoch:41 --- Train_loss:1.0048848390579224 --- Val_loss:1.0652687549591064\n",
      "Epoch:42 --- Train_loss:0.9508607387542725 --- Val_loss:0.9809061884880066\n",
      "Epoch:43 --- Train_loss:0.9600053429603577 --- Val_loss:1.1443111896514893\n",
      "Epoch:44 --- Train_loss:0.9675828814506531 --- Val_loss:1.179380178451538\n",
      "Epoch:45 --- Train_loss:0.9488477110862732 --- Val_loss:0.9090649485588074\n",
      "Epoch:46 --- Train_loss:0.9302062392234802 --- Val_loss:0.929977536201477\n",
      "Epoch:47 --- Train_loss:0.9302729964256287 --- Val_loss:0.9130247831344604\n",
      "Epoch:48 --- Train_loss:0.9985537528991699 --- Val_loss:1.1889214515686035\n",
      "Epoch:49 --- Train_loss:0.9558108448982239 --- Val_loss:0.9411012530326843\n",
      "Epoch:50 --- Train_loss:0.908639669418335 --- Val_loss:1.0886602401733398\n",
      "Epoch:51 --- Train_loss:0.9409645199775696 --- Val_loss:1.0935125350952148\n",
      "Epoch:52 --- Train_loss:0.9064428806304932 --- Val_loss:1.169532060623169\n",
      "Epoch:53 --- Train_loss:0.8504286408424377 --- Val_loss:1.1939237117767334\n",
      "Epoch:54 --- Train_loss:0.8520026206970215 --- Val_loss:0.9803134202957153\n",
      "Epoch:55 --- Train_loss:0.8547306656837463 --- Val_loss:1.1343824863433838\n",
      "Epoch:56 --- Train_loss:0.8460871577262878 --- Val_loss:0.982049286365509\n",
      "Epoch:57 --- Train_loss:0.8341395258903503 --- Val_loss:1.0029805898666382\n",
      "Epoch:58 --- Train_loss:0.8990921378135681 --- Val_loss:0.9813050627708435\n",
      "Epoch:59 --- Train_loss:0.8724393844604492 --- Val_loss:0.911025881767273\n",
      "Epoch:60 --- Train_loss:0.8840845227241516 --- Val_loss:1.158711314201355\n",
      "Epoch:61 --- Train_loss:0.8170511722564697 --- Val_loss:1.0262349843978882\n",
      "Epoch:62 --- Train_loss:0.7933934330940247 --- Val_loss:1.0010981559753418\n",
      "Epoch:63 --- Train_loss:0.7450656890869141 --- Val_loss:1.8765935897827148\n",
      "Epoch:64 --- Train_loss:0.8557767271995544 --- Val_loss:1.14005708694458\n",
      "Epoch:65 --- Train_loss:0.7793739438056946 --- Val_loss:0.9539881944656372\n",
      "Epoch:66 --- Train_loss:0.8770029544830322 --- Val_loss:1.152808427810669\n",
      "Epoch:67 --- Train_loss:0.9192471504211426 --- Val_loss:1.3224515914916992\n",
      "Epoch:68 --- Train_loss:0.845844030380249 --- Val_loss:1.0504651069641113\n",
      "Epoch:69 --- Train_loss:0.7855029702186584 --- Val_loss:1.0411245822906494\n",
      "Epoch:70 --- Train_loss:0.7740240097045898 --- Val_loss:1.10050630569458\n",
      "Epoch:71 --- Train_loss:0.7739265561103821 --- Val_loss:0.9420679807662964\n",
      "Epoch:72 --- Train_loss:0.6808785796165466 --- Val_loss:1.3341591358184814\n",
      "Epoch:73 --- Train_loss:0.7373363971710205 --- Val_loss:1.0244829654693604\n",
      "Epoch:74 --- Train_loss:0.7409617900848389 --- Val_loss:1.185455560684204\n",
      "Epoch:75 --- Train_loss:0.8592734336853027 --- Val_loss:1.3283653259277344\n",
      "Epoch:76 --- Train_loss:0.7556317448616028 --- Val_loss:1.1046712398529053\n",
      "Epoch:77 --- Train_loss:0.8284146785736084 --- Val_loss:1.5405325889587402\n",
      "Epoch:78 --- Train_loss:0.8915165066719055 --- Val_loss:1.2752450704574585\n",
      "Epoch:79 --- Train_loss:0.7436121106147766 --- Val_loss:1.0433857440948486\n",
      "Epoch:80 --- Train_loss:0.6337981820106506 --- Val_loss:1.1651878356933594\n",
      "Epoch:81 --- Train_loss:0.6303578615188599 --- Val_loss:0.9354745149612427\n",
      "Epoch:82 --- Train_loss:0.6831808090209961 --- Val_loss:1.4208519458770752\n",
      "Epoch:83 --- Train_loss:0.6270219087600708 --- Val_loss:1.2065999507904053\n",
      "Epoch:84 --- Train_loss:0.5892372727394104 --- Val_loss:1.0817739963531494\n",
      "Epoch:85 --- Train_loss:0.5732516646385193 --- Val_loss:1.4557104110717773\n",
      "Epoch:86 --- Train_loss:0.7169252038002014 --- Val_loss:1.4431179761886597\n",
      "Epoch:87 --- Train_loss:0.7152047753334045 --- Val_loss:1.098993182182312\n",
      "Epoch:88 --- Train_loss:0.5594614148139954 --- Val_loss:1.2153732776641846\n",
      "Epoch:89 --- Train_loss:0.6122676730155945 --- Val_loss:1.2010937929153442\n",
      "Epoch:90 --- Train_loss:0.679427444934845 --- Val_loss:0.9638941287994385\n",
      "Epoch:91 --- Train_loss:0.6685207486152649 --- Val_loss:1.7122199535369873\n",
      "Epoch:92 --- Train_loss:0.7370452880859375 --- Val_loss:1.8766871690750122\n",
      "Epoch:93 --- Train_loss:0.6818850040435791 --- Val_loss:1.1184484958648682\n",
      "Epoch:94 --- Train_loss:0.5465340614318848 --- Val_loss:1.1731758117675781\n",
      "Epoch:95 --- Train_loss:0.5315960049629211 --- Val_loss:1.0904133319854736\n",
      "Epoch:96 --- Train_loss:0.5908820033073425 --- Val_loss:1.4393575191497803\n",
      "Epoch:97 --- Train_loss:0.548655092716217 --- Val_loss:1.1591018438339233\n",
      "Epoch:98 --- Train_loss:0.5438104271888733 --- Val_loss:1.2892241477966309\n",
      "Epoch:99 --- Train_loss:0.5641049742698669 --- Val_loss:1.573932409286499\n"
     ]
    }
   ],
   "source": [
    "trained_net, training_loss, val_loss = train_network('patient7.npy', features='afterX', label_type='all', label= 'type', nepochs=100, verbose=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = ecogDataset('patient7.npy', features='afterX', label_type='all', label= 'type', split='test')\n",
    "acc, dbg = test_model(trained_net, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6923076923076923"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1, 1]),\n",
       " tensor([1, 1]),\n",
       " tensor([1, 1]),\n",
       " tensor([1, 0]),\n",
       " tensor([1, 1]),\n",
       " tensor([1, 1]),\n",
       " tensor([0, 0]),\n",
       " tensor([1, 0]),\n",
       " tensor([0, 0]),\n",
       " tensor([0, 1]),\n",
       " tensor([2, 1]),\n",
       " tensor([0, 1]),\n",
       " tensor([2, 1])]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
