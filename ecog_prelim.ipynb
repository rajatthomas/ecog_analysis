{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_files = ['Japan_Patient7_PainRatingTask_InsulaB49.csv',\n",
    "                'Japan_Patient7_PainRatingTask_InsulaB50.csv',\n",
    "                'Japan_Patient7_PainRatingTask_InsulaB51.csv',\n",
    "                'Japan_Patient7_PainRatingTask_InsulaB52.csv',\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the csv files into numpy array\n",
    "#col_names = [f'BeforeXpression_{i}' for i in range(1000)] + [f'AfterXpression_{i}' for i in range(1000)] + ['StimType', 'StimIntensity', 'SubjRating' ]\n",
    "ntrials = 252\n",
    "ncols = 2003\n",
    "nelectrodes = 4\n",
    "\n",
    "all_data = np.zeros((ntrials, ncols, nelectrodes))\n",
    "for i, f in enumerate(rating_files):\n",
    "    all_data[:,:,i] = pd.read_csv(file_rating, header=None).values\n",
    "\n",
    "np.save('patient7.npy', all_data)\n",
    "\n",
    "#df.index = [f'Trial_{i}' for i in range(252)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(252, 10, 4)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[:,range(10,20),:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ecogDataset(Dataset):\n",
    "    def __init__(self, data_file, features='afterX', label='type', split='train'):\n",
    "        '''\n",
    "        data_file = '.npy file [ntrials X ncols[all features + labels] X nelectrodes ]'\n",
    "        features = 'beforeX' | 'afterX' | 'all'\n",
    "        label = 'type' | 'intensity' | 'rating'\n",
    "        split = 'train' | 'val' | 'test'\n",
    "        '''\n",
    "        \n",
    "        data = np.load(ecog_response_file)\n",
    "        ntrials, _, _ = data.shape\n",
    "        \n",
    "        ntrain = int(0.8 * ntrials)\n",
    "        nval = int(0.1 * ntrials)\n",
    "        \n",
    "        if features == 'beforeX':\n",
    "            select_features = range(0, 1000)\n",
    "        if features == 'afterX':\n",
    "            select_features = range(1000, 2000)\n",
    "        if features == 'all':\n",
    "            select_features = range(0, 2000)\n",
    "                \n",
    "        if label == 'type':\n",
    "            select_label = 2001\n",
    "        if label == 'intensity':\n",
    "            select_label = 2002\n",
    "        if label == 'rating':\n",
    "            select_label = 2003\n",
    "        \n",
    "        if split == 'train':\n",
    "            select_trials = range(0, ntrain)\n",
    "        if split == 'val':\n",
    "            select_trials = range(ntrain, ntrain+nval)\n",
    "        if split == 'test':\n",
    "            select_trials = range(ntrain+nval, ntrials)\n",
    "        \n",
    "        \n",
    "        self_features = data[select_trials, select_features, :]\n",
    "        self_label = data[select_label]    \n",
    "    \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self_features.shape[0] # num trials\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index], self.label[index]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rating1DConvNet(nn.Module):\n",
    "    def __init__(self, nROIS=2):\n",
    "        super(Rating1DConvNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(nROIS, 16, 7)\n",
    "        self.conv2 = nn.Conv1d(16,32, 5)\n",
    "        self.conv3 = nn.Conv1d(32, 64, 5)\n",
    "        self.avg = nn.AdaptiveAvgPool1d((1))\n",
    "        \n",
    "        self.linear1 = nn.Linear(64, 100)\n",
    "        self.linear2 = nn.Linear(100, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.avg(x).view(-1, 64)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(net, val_data, analysis_type, nTime_min=150, criterion=nn.CrossEntropyLoss()):\n",
    "    \n",
    "    val_data_loader = DataLoader(val_data, batch_size=256, shuffle=True)\n",
    "    \n",
    "    net.eval()\n",
    "    loss = 0.0\n",
    "    for i, (tc, corr, dx) in enumerate(val_data_loader):\n",
    "\n",
    "            if analysis_type == 'tc':\n",
    "                vals = Variable(tc).type(torch.FloatTensor)\n",
    "            if analysis_type == 'corr':\n",
    "                vals = Variable(corr).type(torch.FloatTensor)\n",
    "\n",
    "            dx = Variable(dx).type(torch.LongTensor)\n",
    "            \n",
    "            # forward pass\n",
    "            output = net(vals)\n",
    "\n",
    "            # calculate loss\n",
    "            loss += criterion(output, torch.max(dx,1)[1])\n",
    "\n",
    "    return loss/len(val_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(atlas_name, bptf, confounds, analysis_type ='tc', nTime_min=150, nepochs=100, verbose=True):\n",
    "    \n",
    "    \n",
    "    train_data = AbideData(atlas_name=atlas_name, \n",
    "                           bptf=bptf, \n",
    "                           confounds=confounds,\n",
    "                           nTime_min=nTime_min, split='train')\n",
    "    train_data_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "    \n",
    "    val_data = AbideData(atlas_name=atlas_name, nTime_min=nTime_min, split='val')\n",
    "\n",
    "    nrois = train_data.__getitem__(0)[0].shape[0] # Trick to get the nrois (=nchannels)\n",
    "    N_corr = int(nrois*(nrois+1)/2)\n",
    "    \n",
    "    if analysis_type == 'tc':\n",
    "        net = Abide1DConvNet(nROIS=nrois)\n",
    "    if analysis_type == 'corr':\n",
    "        net = AbideCorrDense(corr_dim=N_corr)\n",
    "        \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=.001, weight_decay=0)\n",
    "    \n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    \n",
    "    print(f'Training ...')\n",
    "    for i_epoch in range(nepochs): \n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        for i, (tc, corr, dx) in enumerate(train_data_loader):\n",
    "\n",
    "            if analysis_type == 'tc':\n",
    "                vals = Variable(tc).type(torch.FloatTensor)\n",
    "            if analysis_type == 'corr':\n",
    "                vals = Variable(corr).type(torch.FloatTensor)\n",
    "            \n",
    "            dx = Variable(dx).type(torch.LongTensor)\n",
    "\n",
    "            # forward pass\n",
    "            output = net(vals)\n",
    "\n",
    "            # calculate loss\n",
    "            loss = criterion(output, torch.max(dx,1)[1])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss\n",
    "\n",
    "        epoch_train_loss = epoch_loss/i\n",
    "        epoch_val_loss = validate_model(net, val_data, analysis_type, nTime_min=nTime_min, criterion=criterion)\n",
    "        \n",
    "        train_loss.append(epoch_train_loss)\n",
    "        val_loss.append(epoch_val_loss)\n",
    "        \n",
    "        if verbose and i_epoch%1 == 0:\n",
    "            print('Epoch:{} --- Train_loss:{} --- Val_loss:{}'.format(i_epoch, epoch_train_loss, epoch_val_loss))\n",
    "            \n",
    "    return net, train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(net, analysis_type, nTime_min=150):\n",
    "    \n",
    "    test_data = AbideData(atlas_name='schaefer_100', nTime_min=nTime_min, split='test')\n",
    "    test_data_loader = DataLoader(test_data, batch_size=16, shuffle=True)\n",
    "    \n",
    "    net.eval()\n",
    "    tot_acc = 0.0\n",
    "    debug_out = []\n",
    "    for i, (tc, corr, dx) in enumerate(test_data_loader):\n",
    "\n",
    "            if analysis_type == 'tc':\n",
    "                vals = Variable(tc).type(torch.FloatTensor)\n",
    "            if analysis_type == 'corr':\n",
    "                vals = Variable(corr).type(torch.FloatTensor)\n",
    "\n",
    "            dx = Variable(dx).type(torch.LongTensor)\n",
    "            \n",
    "            # forward pass\n",
    "            output = net(vals)\n",
    "\n",
    "            debug_out.append(output)\n",
    "            # calculate accuracy\n",
    "            tot_acc += sum((torch.argmax(output,1) == torch.argmax(dx,1)).type(torch.FloatTensor))\n",
    "            \n",
    "    return tot_acc/test_data.__len__(), debug_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_net, training_loss, val_loss = train_network(atlas_name='schaefer_100', bptf=True, confounds=True, analysis_type='corr', nepochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
